	<!DOCTYPE HTML>

<html>
	<head>
		<title>Portfolio Artur Skrzeta</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><a href="https://arturskrzeta.github.io/"><img src="images/avatar.png" alt="" style="margin-bottom:0px"/></a></span>
					<h1 id="logo"><a href="https://arturskrzeta.github.io/">Artur Skrzeta</a></h1>
					<p>Data Analyst<br />
					+5 years experience</p>
				</header>
				<nav id="nav">
					<ul>
						<li><a href="#one" class="active">Intro</a></li>
						<li><a href="#two">Features</a></li>
						<li><a href="#three">Demo</a></li>
						<li><a href="#four">Setup</a></li>
						<li><a href="#five">Source Code</a></li>
					</ul>
				</nav>
				<footer>
					<ul class="icons">
						<li><a href="https://www.linkedin.com/in/artur-skrz%C4%99ta-010b23187/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://www.instagram.com/arturskrrr/" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
						<li><a href="https://github.com/ArturSkrzeta" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="mailto: arturskrzeta@gmail.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li
					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="container">
									<header class="major">

										<h2>OLAP in Snowflake</h2>
										<h3>Intro</h3>
										<p style="text-align: justify">Snowflake is the platform delivered as a service which means we don't have to worry about patches, upgrades or maintenance.
										I like the way they describe it: <i>combines the power of data warehousing, the flexibility of big data platforms, and the elasticity of the cloud</i>.</p>

										<h5>Navigate to Snowflake aspects:</h5>
										<ul>
											<li><a href="#snowflake_architecture">Snowflake architecture</a></li>
											<li><a href="#snowflake_pipeline">Snowflake pipeline</a></li>
											<li><a href="#snowflake_objects">Snowflake objects</a></li>
											<li><a href="#olap">Online Analytical Processing</a></li>
											<li><a href="#snowflake_workflow">Snowflake workflow</a></li>
											<li><a href="#sql_scripting">Sql scripting</a></li>
											<li><a href="#data_load">Data load</a></li>
											<li><a href="#snow_sql">SnowSQL</a></li>
											<li><a href="#snow_pipe">SnowPipe</a></li>
											<li><a href="#time_travel">Time Travel</a></li>
											<li><a href="#bulk_unloading">Bulk Unloading</a></li>
											<li><a href="#update_statement">Update Statement</a></li>
											<li><a href="#stored_procedure">Stored procedure</a></li>
											<li><a href="#variables">Variables</a></li>
										</ul>

										<!-- <h5>6. Continuous monitoring:</h5>
										<ul>
											<li>...</li>
											firewall configures some security rules. Accepting connection from trusted network and blocking connection from untrusted network.
										</ul> -->

									</header>
								</div>
							</section>

						<!-- Two -->
							<section id="two">
								<div class="container">
									<h3>Features</h3>
									<p>App includes following features:</p>
									<ul class="feature-icons">
										<li class="icon solid fa-check">Snowflake</li>
									</ul>
								</div>
							</section>

						<!-- Three -->
							<section id="three">
								<div class="container">
									<h3>Demo</h3>
									<h5 id="snowflake_architecture">Snowflake architecture</h5>
									<ul>
										<li>In Snowflake's architecture, the key is a separation of storage, compute and services:
											<br><br>
											<img src="images/snowflake_architecture.png" width="640"><span style="font-size:12px">soruce: snowflake</span>
											<br>
											- S3 as a storage layer (Google Cloud Storage or Azure Blob Storage),<br>
											- EC2 Virtual Warehouse as a compute layer .<br>
											- metadata, security, transactions and concurrency management as service layer.
										</li>
										<li id="snowflake_pipeline">Building pipelines with Snowflake:
											<br><br>
											<img src="images/snowflake_pipeline.JPG">
											<span style="font-size:12px">soruce: snowflake</span>
										</li>
										<li id="snowflake_objects">Snowflake objects:
												<br>
												- Database - containing schemas, tables, views, stages, file formats, sequences. All the tables are created in PUBLIC schema by default.<br>
												- Stage - a data lake like place from which data files are being copied into the tables.<br>
												- File format - we need to define format of the files bieng stored at stage.<br>
												- Warehouse - computing enginge.<br>
												- Worksheet - a place where we can proceed with writing or loading ready SQL scripts.<br>
										</li>
									</ul>

									<h5>Snowflake mechanism:</h5>
									<ul>
										<li id="olap">Snowflake for OLAP:
											<br>
											- snowflake supports <b>Online Analytical Processing (OLAP)</b> for scanning all of the data in columns in order to get aggregated values as opposed to <b>Online Transactional Processing (OLTP)</b> where we want to retrieve one or many rows from database.
												The main purpose of OLAP is data analysis and when it comes to OLTP then main purpose is data processing (inserting, updating).
											<br><br>
											<img src="images/olap.JPG" style="border: 1px solid gray;" width="420"><span style="font-size:12px">soruce: Data Council</span>
											<br>
											- while a classic databases like PostgreSQL or MySQL are type of <b>raw store</b> (OLTP) that keeps data linearly row by row, from left to right, from top to bottom, Snowflake supports <b>columnar store</b> (OLAP) which allows to traverse entire columns one by one.
											<br><br>
											<img src="images/columnstore.JPG" style="border: 1px solid gray;" width="640"><span style="font-size:12px">soruce: Data Council</span>
											<br>
											- using columnar storage makes sense when we want to analyze one or two specific columns and skip reading other ones.
										</li>
										<li id="snowflake_workflow">Snowflake workflow:
											<br>
											- Dividing big tabular data into x-consecutive rows segments called micropartitions.<br>
											- Reorganizing the data of each micropartition by flipping row store into columnar - column values of each row get stored together.<br>
											- Adding a header to each micropartition. Header contains metadata of given micropartition and columns it contains.<br>
											- Micropartitions are being stored in S3 as the immutable files.<br>
											- All the information about micropartitions are being stored in Sknowflake's metadata layer.<br>
											- When query comes in, metadata layer knows which micropartition in S3 belongs to the queried table.<br>
											- Reading a S3 file, it searches for a specific header first and then reads the only column we specified in the query.<br>
											- It reads the only queried data ingnoring the rest of S3 files.
										</li>
										<li id="sql_scripting">Every action on db can be performed with sql script along with setting up stages, file formats,
												configuring warehouse, copying data to a table from a stage.
												<br><br>
												<img src="images/sql_script.JPG">
												<br>
												<img src="images/sql_script2.JPG">
										</li>
										<li id="data_load">Data Loading consists of two parts:
											<br>
											1. Staging the files into the place where Snowflake can retrieve data files from. We can have both internal and exteranl (S3, GSP) storages.<br>
											2. Loding the data into a specific table.
										</li>
									</ul>

									<h5id="snow_sql">SnowSQL</h5>
									<ul>
										<li>The command line interface client for connecting to Snowflake.</li>
										<li>Allows to execute SQL queries to perform DDL and DML operations</li>
										<li>We can install it from Snowflake UI and there is info on how to connect at the end:
											<br><br>
											<img src="images/snowsql_installation.JPG">
											<br>
										<li>Example:
											<br>
											<code>USE DATABASE my_db;</code><br>
											<code>CREATE OR REPLACE TABLE my_tbl {</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>...</code><br>
											<code>};</code><br>
											<code>CREATE STAGE csvfiles;</code><br>
											<code>PUT file:///tmp/load/abc*.csv @csvfiles;</code><br>
											<code>LIST @csvfiles;</code><br>
											<code>CREATE WAREHOUSE IF NOT EXISTS "dataload" WAREHOUSE.SIZE = "Large"</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>AUTOSUSPEND = 300 AUTO_RESUME = TRUE;</code><br>
											<code>USE WAREHUSE dataload;</code><br>
											<code>COPY INTO my_tbl</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>FROM @csvfiles</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>PATTERN = '.*file0[1-4].csv.gz'</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>ON_ERROR = 'skip_file';</code><br>
											<code>SELECT * FROM my_tbl LIMIT 10;</code>
										</li>
										<li>We can learn a lot from the documentation:
												<u><a href="https://docs.snowflake.com/en/">Snowflake documentation</a></u>
										</li>
									</ul>

									<h5 id="snow_pipe">SnowPipe:</h5>
									<ul>
										<li>It loads data from a sources like AWS S3 bucket into a certain target table.</li>
										<li>With SnowPipe we can perform whole ETL process:
<!-- start -->
<pre>
<span style="color:#00ff45">CREATE PIPE</span> my_pipe
<span style="color:#00ff45">AUTO_INGEST</span> = FALSE
<span style="color:#00ff45">AS</span>
<span style="color:#00ff45">COPY INTO</span> "MY_DB"."MY_STAGE"."tbl_my_table" (col1, col2, col3, col4, col5, col6, col7, col8)
  <span style="color:#00ff45">FROM</span> (
    <span style="color:#00ff45">SELECT</span>
      <span style="color:#00ffea">substr</span>(stage_alias.$3, 1, 7),
      <span style="color:#00ffea">iff</span>(stage_alias.$1 = 100, replace(stage_alias, '100', 'Done'), 'In progress'),
      stage_alias.$2,
      stage_alias.$3,
      <span style="color:#00ffea">replace</span>(stage_alias.$5, ',', '.'),
      'Artur',
      metadata$filename,
      <span style="color:#00ffea">CURRENT_TIMESTAMP</span>()
    <span style="color:#00ff45">FROM</span>
      @"MY_DB"."MY_STAGE"."s3_bucket_name" stage_alias
    )
<span style="color:#00ff45">FILE FORMAT</span> = (format_name = "MY_DB"."MY_STAGE"."SAP_X")
<span style="color:#00ff45">FORCE</span> = FALSE;
</pre>
<!-- end -->
											- at first, we create a pipe name and assign it to the COPY INTO,<br>
											- each pipe definition starts with COPY INTO then we state a destination table according to the path:<br>
											<code>database</code>.<code>stage</code>.<code>destination table</code>,<br>
											then we list all the destination columns that we want to feed with the data load.<br>
											- In SELECT statement, we perform entire data manipulation where each line corresponds to a signle column listed in the COPY INTO line,<br>
											- In SELECT statement, we work with indexes which refer to a column number in the data source,<br>
											- In SELECT statement, we can jump over an index if we want to avoid some columns or we can manipulate the data source columns sequence,<br>
											- In SELECT statement, we can either use source column (original or transformed) or we can give a hard-coded value there,<br>
											- For example the first line in SELECT takes 3rd column from data source and transforms it with substr(),<br>
											- Using metadata$filename gives us the name of current file's name,<br>
											- with FILE FORMAT we tell Snowflake what is the structure of the data source, what is the delimiter there etc,<br>
											- with FORCE equal to FALSE we tell Snowflake not to import the file agian if it was imported before.
										</li>
										<li>AWS stage:
											<br>
											- in the example above we use the external stage of AWS S3,<br>
											- to create the stage we need to provide S3 bucket's url along with credentials,<br>
<!-- start -->
<pre>
<span style="color:#00ff45">CREATE OR REPLACE STAGE</span> "MY_DB"."MY_SCHEMA"."stage_name"
  <span style="color:#00ff45">URL</span> = 's3://bucket_name/folder_name'
  <span style="color:#00ff45">CREDENTIALS</span> = (aws_key_id='.....' aws_secret_key='.....')
  <span style="color:#00ff45">ENXRYPTION</span> = (type='AWS_SSE_KMS' kms_key_id = '......');
</pre>
<!-- end -->
											- encryption is being used in order to encrypt the confidential data.<br>
											- instead of passing credentials to each stage we can move on with so-called STORAGE INTEGRAION:<br>
<!-- start -->
<pre>
<span style="color:#00ff45">CREATE STORAGE INTEGRATION</span> "MY_DB"."MY_SCHEMA"."AWS_111111_Integration"
<span style="color:#00ff45">TYPE</span> = EXTERNAL_STAGE
<span style="color:#00ff45">STORAGE_PROVIDER</span> = S3
<span style="color:#00ff45">ENABLED</span> = TRUE
<span style="color:#00ff45">STORAGE_AWS_ROLE_ARN</span> = 'arn:aws:iam::111111:role/my_role'
<span style="color:#00ff45">STORAGE_ALLOWED_LOCATIONS</span> = (
  's3://bucket_name/',
  's3://another_bucket_name/')
</pre>
<!-- end -->
											- which we can use in CREAT OR REPLACE STAGE script:<br>
<!-- start -->
<pre>
<span style="color:#00ff45">CREATE OR REPLACE STAGE</span> "MY_DB"."MY_STAGE"."stage_name"
  <span style="color:#00ff45">URL</span> = 's3://bucket_name/folder_name'
  <span style="color:#00ff45">STORAGE_INTEGRATION</span> = "MY_DB"."MY_SCHEMA"."AWS_111111_Integration"
</pre>
<!-- end -->
											- ARN stands for Amazon Resource Name.
										</li>
										<li>With the snowpipe we can understand what are the all dependencies and refreneces to the Snowflake objects based on the source code itself.</li>
									</ul>

									<h5 id="time_travel">Time Travel:</h5>
									<ul>
										<li>Every time a DML operation is executed on a table, Snowflake saves previous versions of the table data. This allows to query earlier data versions using the AT | BEFORE clause.</li>
										<li>The data retention period specifies the number of days for which this historical data is preserved and, therefore, Time Travel operations
												(SELECT, CREATE … CLONE, UNDROP) can be performed on the data. The standard retention period is 1 day (24 hours). When the retention
												period ends for an object, the historical data is moved into Snowflake Fail-safe (background process).
										</li>
										<li>The data retention period can be modified up to 90 days. It can be overridden using the DATA_RETENTION_TIME_IN_DAYS while creating table, schema or database.
											<br>
											<code>create table mytable(col1 number, col2 date) data_retention_time_in_days=90;</code><br>
											<code>alter table mytable set data_retention_time_in_days=30;</code><br>
											- If a retention period is specified for a database or schema, the period is inherited by default for all objects created in the database/schema.
										</li>
										<li>We can query a specified point in the table’s history within the retention period giving timestamp, offset from a current date or giving statement query ID.
<!-- start -->
<pre>
<span style="color:#00ff45">-- timestamp</span>
select * from my_table at(timestamp => 'Mon, 01 May 2015 16:20:00 -0700'::timestamp_tz);

<span style="color:#00ff45">-- time offset from current date</span>
select * from my_table at(offset => -60*5);

<span style="color:#00ff45">-- statement id</span>
select * from my_table before(statement => '8e5d0ca9-005e-44e6-b858-a8f5b37c5726');
</pre>
<!-- end -->
											- Querying outside the data retention period for the table, the query fails and returns an error.
										</li>
									</ul>

									<h5 id="bulk_unloading">Bulk Unloading</h5>
									<ul>
										<li>We can use <code>COPY INTO location from tbl_name</code> command to copy the data from the Snowflake database table into one or more files in a Snowflake or external stage.</li>
										<li>We can also unload the data into S3 Bucket directly:
											<br>
											<code>copy into 's3://bucket_name/folder_name/'</code><br>
											<code>from mytable_name</code><br>
											<code>storage_integration = s3_int_name;</code>
											<br>
											- we must specify the URI for the S3 bucket and the storage integration or credentials for accessing the bucket in the COPY command:
											<br>
											<code>COPY INTO 's3://mybucket/folder_name/'</code><br>
											<code>from my_table</code><br>
											<code>credentials = (aws_key_id='xxxx' aws_secret_key='xxxxx' aws_token='xxxxxx')</code>
										</li>
										<li>Unloading to a File:
											<br>
<!-- start -->
<pre>
copy into @mystage/myfile.csv.gz from mytable
file_format = (type=csv compression='gzip')
single=true
max_file_size=4900000000;
</pre>
<!-- end -->
											<span style="font-size:12px;">source: docs.snowflake.com</span><br>
											- loading table data to a single file named myfile.csv in a named stage.
										</li>
										<li>Bulk unloading with the stored procedure:
<!-- start -->
<pre>
CREATE OR REPLACE PROCEDURE "db_name"."db_schema".unload_procedure()
RETURNS string
LANGUAGE javascript
AS
$$

    var <span style="color:#00ff45">tbl_arr</span> = [
                    'tbl_clients',
                    'tbl_products',
                    'tbl_vendors',
                    'tbl_employees',
                    'tbl_discounts',
                    'tbl_premium_products',
                 ];

    var <span style="color:#00ff45">tbl_arr_length</span> = tbl_arr.length;

    var <span style="color:#00ff45">today</span> = new Date();
    var <span style="color:#00ff45">date</span> = today.getFullYear()+'-'+(today.getMonth()+1)+'-'+today.getDate();

    for (var i = 0; i < tbl_arr_length; i++){

      var <span style="color:#00ff45">table_name</span>  = tbl_arr[i];
      var <span style="color:#00ff45">file_name</span>  = <span style="color:#00ff45">date</span> + "_" + <span style="color:#00ff45">table_name</span>;

      var <span style="color:#00ff45">s3_bucket_url</span>  = 's3://my_backup/db-backup/' + <span style="color:#00ff45">file_name</span>;
      var <span style="color:#00ff45">full_table_name</span>  = '"db_name"."db_schema"."' + <span style="color:#00ff45">table_name</span> + '"';
      var <span style="color:#00ff45">storage_integration_name</span>  = '"db_name.db_schema.AWS_12345_Integration"'

      var <span style="color:#00ff45">sql</span>  = "";
      <span style="color:#00ff45">sql</span>  += "COPY INTO " + <span style="color:#00ff45">s3_bucket_url</span> + " ";
      <span style="color:#00ff45">sql</span>  += "FROM " + <span style="color:#00ff45">full_table_name</span> + " ";
      <span style="color:#00ff45">sql</span>  += "STORAGE_INTEGRATION = " + <span style="color:#00ff45">storage_integration_name</span> + " ";
      <span style="color:#00ff45">sql</span>  += "FILE_FORMAT = ( TYPE = CSV null_if=('Null') field_optionally_enclosed_by='\"');";

      var <span style="color:#00ff45">sql_command</span>  = snowflake.createStatement({sqlText: <span style="color:#00ff45">sql</span>});

      try {
          var <span style="color:#00ff45">sql_result</span>  = <span style="color:#00ff45">sql_command</span>.execute();
      }
      catch (<span style="color:#00ff45">e</span>) {
          return = 'bulk unloading failed: ' + <span style="color:#00ff45">e</span>;
      };

    };

    result = 'DONE'
    return <span style="color:#00ff45">result</span> ;

$$;
</pre>
<!-- end -->
											- the procedure loops through all of the tables in the database,<br>
											- within the loop, it gives a file a name of the table along with the timestamp beign concatinated which is meant to build the S3 bucket's url,<br>
											- the rest of the loop is responsible for buildin the sql statement and executing it within try-catch block.
										</li>
									</ul>

									<h5 id="update_statement">Update statement:</h5>
									<ul>
										<li>When update statement sent to a cloud database, there is no change to an existing record. Actually, a new record is being inserted with an updated value(s) instead.
											There is also the metadata that controlls which one of those records is the most updated. The old records are being kept in the storage and kept available so that we can go back to these if needed.
										</li>
										<li>This is unlike sql sever database for example, where update statement changes the records value in place.</li>
									</ul>

									<h5 id="stored_procedure">Stored procedures:</h5>
									<ul>
										<li>A stored procedure returns a single value only.</li>
										<li>Although we run SELECT inside a stored procedure, the results must be used within the stored procedure, or be narrowed to a single value and then returned.</li>
										<li>Language used in JavaScript that provides the control structures (branching and looping) and SQL statements are executed by calling functions in a JavaScript API.</li>
										<li>Example procedure that takes one variable from an user:
<!-- start -->
<pre>
CREATE OR REPLACE PROCEDURE procedure_name (
    <span style="color:#00ff45">project_name</span> varchar
)
RETURNS ARRAY
LANGUAGE javascript
AS
$$

    var query = "SELECT MANAGER_NAME FROM PROJECTS WHERE PROJECT_NAME = ?";
    var statement = snowflake.createStatement({sqlText: query, binds: [<span style="color:#00ff45">PROJECT_NAME</span>]});
    var resultSet = statement.execute();

    var arr = [];

    while (resultSet.next())(
      arr.push(resultSet.GetColumnValue(1));
    )

    return arr;

$$;


CALL procedure_name('asap_project')
</pre>
<!-- end -->
											- When defining a stored procedure we need to declare the type of returned vlaue, here ARRAY.<br>
											- If we want to return different type depending on flow control we can declare returned value as VARIANT.<br>
											- $$ is the delimiter for the beginning and end of the stored procedure.<br>
											- With Javascript API global object's function: snowflake.createStatement() we pass JSON object with two key-value pairs.<br>
											- With binds key we bind '?' in the SELECT statement with passed variables.<br>
											- We can either bind variables with query or we can concat variables in a SELECT statment like:
												<code>"SELECT MANAGER_NAME FROM PROJECTS WHERE</code><br>
												<code>PROJECT_NAME = '" + <span style="color:#00ff45">PROJECT_NAME</span> + "';"</code><br>
											- The variable: resultSet holds the outcome of SELECT statement.<br>
											- In order to access each row from the resultSet, we need to iterate it calling resultSet.next() one time for each record.<br>
											- As far as there are records still available, resultSet.next() returns TRUE in the while loop.<br>
											- With <code>arr.push(resultSet)</code> we put all record's columns into the array.<br>
											- We can narrow down the result to one column only with the following: <code>arr.push(resultSet.GetColumnValue(1)).</code><br>
											- Result of above: <code>['Artur', 'Johnny']</code>
										</li>
										<li>IMPORTANT to note, we can pass variable names into procedrue in either upper or lower case but using these variables inside a procedure we need to use them with upper cases.
												This is because SQL by default converts columns in upper case.
										</li>
										<li>We can execute many sql statements within one procedure:
<!-- start -->
<pre style="">
CREATE OR REPLACE PROCEDURE "db_name"."schema_name".prcedure_name()
RETURNS string
LANGUAGE javascript
AS
$$

// Creating 2 temporary tables:
// 1st one contains all ITEM with a single classifier assigned (there may be duplicates)
// 2nd one contains unique ITEM with order_status defined

//  Creating temporary table with all classifiers per ITEM

    var tbl_products = '"db_name"."schema_name"."tbl_products"';
    var tbl_tools = '"db_name"."schema_name"."tbl_tools"';
    var tbl_items = '"db_name"."schema_name"."tbl_items"';
    var tbl_things = '"db_name"."schema_name"."tbl_things"';
    var tbl_spare_parts = '"db_name"."schema_name"."tbl_spare_parts"';

    var tbl_temporary_name = '"db_name"."schema_name"."tbl_all_stuff"';

    var tbl_temporary_definition = "";
    tbl_temporary_definition += "CREATE OR REPLACE TABLE " + tbl_temporary_name + " AS ";
    tbl_temporary_definition += "SELECT *, SUM(CASE WHEN CATEGORY = 'Spare' THEN 1 ELSE 0 END)
		 	OVER(PARTITION BY ITEM ORDER BY ITEM) AS IF_SPARE_SOMEWHERE,
			COUNT(*) OVER(PARTITION BY ITEM ORDER BY ITEM) ITEM_COUNT FROM("
    tbl_temporary_definition += "SELECT ITEM, CATEGORY, PART_NUMBER, DATA_SRC, DATA_SRC_FILENAME
		 	FROM " + tbl_products + " ";
    tbl_temporary_definition += "UNION ";
    tbl_temporary_definition += "SELECT ITEM, CATEGORY, PART_NUMBER, DATA_SRC, DATA_SRC_FILENAME
		 	FROM " + tbl_tools + " ";
    tbl_temporary_definition += "UNION ";
    tbl_temporary_definition += "SELECT ITEM, CATEGORY, PART_NUMBER, DATA_SRC, DATA_SRC_FILENAME
		 	FROM " + tbl_items + " ";
    tbl_temporary_definition += "UNION ";
    tbl_temporary_definition += "SELECT ITEM, CATEGORY, PART_NUMBER, DATA_SRC, DATA_SRC_FILENAME
		 	FROM " + tbl_things + " ";
    tbl_temporary_definition += "UNION ";
    tbl_temporary_definition += "SELECT ITEM, 'Spare', PART_NUMBER, DATA_SRC, DATA_SRC_FILENAME
		 	FROM " + tbl_spare_parts;
    tbl_temporary_definition += ") ORDER BY 1;";

    // example
    // ------------------------------------------------------------------------------
    // ITEM |   Classifier      | ... | DATA_SRC    | ... |   IF_SPARE_SOMEWHERE   	|
    // ------------------------------------------------------------------------------
    // 111  |   Spare           | ... | ERP         | ... |   0                     |
    // ------------------------------------------------------------------------------
    // 123  |   Spare           | ... | ERP         | ... |   0                     |
    // 123  |   different       | ... | ERP         | ... |   0                     |
    // 123  |   different_2     | ... | ERP         | ... |   0                     |
    // ------------------------------------------------------------------------------
    // 999  |   something       | ... | ERP         | ... |   1                     |
    // 999  |   something_2     | ... | ERP         | ... |   1                     |
    // 999  |   Spare           | ... | SSYSTEM     | ... |   1                     |

    var sql_command = snowflake.createStatement({sqlText: tbl_temporary_definition});

    try {
        var sql_result = sql_command.execute();
    }
    catch (e) {
        return 'creating temporary table failed: ' + e;
    }

// deleting duplicated ITEM when coming from both SAP and SUPER_SYSTEM (deleting SAP ones)
// prioritizing SUPER_SYSTEM entires
// due to example - it will remove two records with ITEM of 999 where DATA_SRC is ERP

    var delete_sql_statement = "";
    delete_sql_statement += "DELETE ";
    delete_sql_statement += "FROM " + tbl_temporary_name + " ";
    delete_sql_statement += "WHERE ITEM_COUNT > 1 AND CATEGORY != 'Spare' AND IF_SPARE_SOMEWHERE = 1;";

    var sql_command = snowflake.createStatement({sqlText: delete_sql_statement});

    try {
        var sql_result = sql_command.execute();
    }
    catch (e) {
        return 'deleting ERP duplicates from tbl_all_stuff failed: ' + e;
    }

// dropping ITEM_COUNT_COLUMN and IF_CURRIMA_SOMEWHERE from tbl_all_stuff

    var drop_column = "";
    drop_column += "ALTER TABLE " + tbl_temporary_name + " ";
    drop_column += "DROP COLUMN ITEM_COUNT, IF_SPARE_SOMEWHERE;";

    var sql_command = snowflake.createStatement({sqlText: drop_column});

    try {
        var sql_result = sql_command.execute();
    }
    catch (e) {
        return 'droping column from tbl_all_stuff failed: ' + e;
    }

    // example:
    // --------------------------------------------------
    // ITEM   |   Classifier      | ... | DATA_SRC  | ...
    // --------------------------------------------------
    // 111    |   Spare           | ... | ERP       | ...
    // --------------------------------------------------
    // 123    |   Spare           | ... | ERP       | ...
    // 123    |   different       | ... | ERP       | ...
    // 123    |   different_2     | ... | ERP       | ...
    // --------------------------------------------------
    // 999    |   Spare           | ... | SSYSTEM   | ...

//  Insert for testing -> ITEM 666111 should appear in the tbl_items_master at the end of procedure - if so then test passed - omment out when want to skip
//    var insert_text = "INSERT INTO " + tbl_temporary_name +
	" (ITEM, CATEGORY, PART_NUMBER, DATA_SRC) VALUES('666111', 'Spare', '12345', 'SSYSTEM_B')";
//    var stmt_insertIntoTblProjectsMaster_command = snowflake.createStatement({sqlText: insert_text});
//    var res = stmt_insertIntoTblProjectsMaster_command.execute();

//  Creating table with unique item and order_status

    var tbl_temporary_unique_item_name = '"db_name"."schema_name"."tbl_temp_unique_item"';

    // everything from super_system is open and has higher priority than SAP entieris
    // (sap ones should be dropped when diplicate super_system ones)

    var case_logic = ""
    case_logic += "CASE ";
    case_logic += "WHEN CATEGORY = 'Spare' THEN 'open' ";
    case_logic += "WHEN if_spare >= 1 AND count > 1 THEN 'open' ";
    case_logic += "WHEN if_spare = 1 AND count = 1 THEN 'to be checked' ";
    case_logic += "WHEN if_spare = 0 AND count > 0 THEN 'open' ";
    case_logic += "WHEN if_spare = 0 AND count = 0 THEN 'closed' ";
    case_logic += "END ";

    var tbl_temporary_definition = "";
    tbl_temporary_definition += "CREATE OR REPLACE TABLE " + tbl_temporary_unique_item_name + " AS ";
    tbl_temporary_definition += "SELECT DISTINCT ITEM, ";
    tbl_temporary_definition += "SUM(CASE WHEN CATEGORY = 'Spare' THEN 1 ELSE 0 END)
			OVER(PARTITION BY ITEM ORDER BY ITEM) AS if_spare, ";
    tbl_temporary_definition += "COUNT(*) OVER(PARTITION BY ITEM ORDER BY ITEM) AS count, ";
    tbl_temporary_definition += case_logic + " AS order_status ";
    tbl_temporary_definition += "FROM " + tbl_temporary_name;

    var sql_command = snowflake.createStatement({sqlText: tbl_temporary_definition});

    try {
        var sql_result = sql_command.execute();
    }
    catch (e) {
        return 'creating temporary table wiht unique ITEM failed: ' + e;
    }

    // example
    // -------------------------------------------------------
    // ITEM  |   if_spare   |   count   |   order_status
    // -------------------------------------------------------
    // 111  |       1       |   1       |   to be checked
    // -------------------------------------------------------
    // 123  |       1       |   3       |   open
    // -------------------------------------------------------
    // 999  |       0       |   1       |   open
    // -------------------------------------------------------

//  Main
    var tbl_items_master = '"db_name"."schema_name"."tbl_items_master"';
    var tbl_erpdata = "ERP ";
    var insert_text = "INSERT INTO " + tbl_items_master +
			" (ITEM, PART_NUMBER, ORDER_STATUS, ABC, BUDGET, USER_ID, EDIT_TIMESTAMP) ";

    var outer_query = "";
    outer_query += "SELECT DISTINCT erpdata.ITEM, ";
    outer_query += "'none (this record is system-generted based on " +
			tbl_erpdata + " data import)', ";
    outer_query += "erpdata.ORDER_STATUS, ";

    // logic to define ABC:
    // ERP_A - A100 (based on DATA_SRC)
    // ERP_B - B300 (based on DATA_SRC)
    // SSYSTEM_A - A100 (based on DATA_SRC_FILENAME)
    // SSYSTEM_B - B300 (based on DATA_SRC_FILENAME)
    // else - tbd

    outer_query += "iff(erpdata.DATA_SRC like 'ERP%',
			iff(erpdata.DATA_SRC like 'ERP_A', 'A100', 'B300'),
			iff(erpdata.DATA_SRC_FILENAME like 'SSYSTEM%',
			iff(erpdata.DATA_SRC_FILENAME like '%SSYSTEM_A%', 'A100', 'B300'),
			'tbd')), ";
    outer_query += "left(erpdata.PART_NUMBER,5), ";
    outer_query += "'system' user_id, ";
    outer_query += 'current_timestamp()' + " ";

    var inner_query = "";
    inner_query += "SELECT a.*, p.ORDER_STATUS ";
    inner_query += "FROM " + tbl_temporary_name + " a ";
    inner_query += "JOIN " + tbl_temporary_unique_psp_name + " p ";
    inner_query += "ON a.ITEM = p.ITEM";

    // inner_query example result:
    // ---------------------------------------------------------------------------
    // ITEM  | CATEGORY     |   PART_NUMBER  |  DATA_SRC  |  ORDER_STATUS
    // ---------------------------------------------------------------------------
    // 123  | Spare         |        ...        |    ...     |  open
    // ---------------------------------------------------------------------------
    // 123  | different     |        ...        |    ...     |  open
    // ---------------------------------------------------------------------------
    // 123  | different_2   |         ...       |    ...     |  open
    // ---------------------------------------------------------------------------

    var sub_from_text = "FROM (" + inner_query + ") erpdata ";

    var sub_join_text = "FULL OUTER JOIN " + tbl_items_master +
			" projects on projects.ITEM = erpdata.ITEM ";
    var sub_where_text = "WHERE projects.ITEM is null and erpdata.ITEM is not null;";
    var values_text = outer_query + sub_from_text + sub_join_text + sub_where_text;
    var insertIntoTblProjectsMaster_command = insert_text + values_text;

    var stmt_insertIntoTblProjectsMaster_command = snowflake.createStatement(
			{sqlText: insertIntoTblProjectsMaster_command});

    try {
        var res = stmt_insertIntoTblProjectsMaster_command.execute();
    }
    catch (e) {
        return 'inserting to master table failed: ' + e;
    }

//  Dropping tbl_temp_unique_psp
//  comment these out if you want to keep the table for further investigation
    var drop_definition = "DROP TABLE " + tbl_temporary_unique_psp_name;
    var sql_command = snowflake.createStatement({sqlText: drop_definition});
    var sql_result = sql_command.execute();


//  Dropping temporary table
//  comment these out if you want to keep the table for further investigation
    var drop_definition = "DROP TABLE " + tbl_temporary_name;
    var sql_command = snowflake.createStatement({sqlText: drop_definition});
    var sql_result = sql_command.execute();

    result = "Done"
    return result;

$$;

CALL "db_name"."schema_name".prcedure_name()
</pre>
<!-- end -->
										</li>
									</ul>

									<h5 id="variables">Variables</h5>
									<ul>
										<li>We can make an usage of variables:
<!-- start -->
<pre>
SET name = 'Artur'

SHOW VARIABLES;

SELECT *
FROM employees
WHERE employee_name = $name
</pre>
<!-- end -->
											- calling variable we need to precede it with dollar sign.
										</li>
										<li>When variable sores the name of db object we need to wrap it in indetifier() function:
<!-- start -->

<pre>
SET my_table_name = "employees"

SELECT * FROM IDENTIFIER($my_table_name);
DROP TABLE IDENTIFIER($my_table_name);
</pre>
<!-- end -->
										</li>
									</ul>

								</div>
							</section>

						<!-- Four -->
							<section id="four">
								<div class="container">
									<h3>Setup</h3>

									<h5>Following installation required:</h5>
									<ul>
										<li><a href="https://signup.snowflake.com/">Snowflake page.</a></li>
									</ul>

								</div>
							</section>

							<section id="five">
								<div class="container">
									<h3>Source Code</h3>
									<p>You can view the source code: <a href="#">HERE</a></p>
									<p>&nbsp</p>
								</div>
							</section>

					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<a href="https://arturskrzeta.github.io/" style="padding-bottom:10px;">Back to Portfolio</a>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
