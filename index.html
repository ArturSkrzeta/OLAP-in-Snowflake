	<!DOCTYPE HTML>

<html>
	<head>
		<title>Portfolio Artur Skrzeta</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><a href="https://arturskrzeta.github.io/"><img src="images/avatar.png" alt="" style="margin-bottom:0px"/></a></span>
					<h1 id="logo"><a href="https://arturskrzeta.github.io/">Artur Skrzeta</a></h1>
					<p>Data Analyst<br />
					+4 years experience</p>
				</header>
				<nav id="nav">
					<ul>
						<li><a href="#one" class="active">Intro</a></li>
						<li><a href="#two">Features</a></li>
						<li><a href="#three">Demo</a></li>
						<li><a href="#four">Setup</a></li>
						<li><a href="#five">Source Code</a></li>
					</ul>
				</nav>
				<footer>
					<ul class="icons">
						<li><a href="https://www.linkedin.com/in/artur-skrz%C4%99ta-010b23187/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://www.instagram.com/arturskrrr/" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
						<li><a href="https://github.com/ArturSkrzeta" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="mailto: arturskrzeta@gmail.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li
					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="container">
									<header class="major">

										<h2>My Comprehension of AWS Services</h2>
										<h3>Intro</h3>
										<p style="text-align: justify">In order to learn clouds, I choose AWS Web Services as the example to explore.
												The best thing about cloud solutions is that companies can simply rent a server instead of building, maintaining and paying for its own infrastructure.</p>

										<h5>Navigate to AWS aspects:</h5>
										<ul>
											<li><a href="#cloudcomputing">Cloud Computing</a></li>
											<li><a href="#serverconnection">Server Connection</a></li>
											<li><a href="#iaac">Infrastructure as a Code</a></li>
											<li><a href="#awsstorage">AWS storage</a></li>
											<li><a href="#domainnamesystem">Domain Name System</a></li>
											<li><a href="#serverlessservices">Serverless services</a></li>
											<li><a href="#storageclasses">Storage classes</a></li>
										</ul>

										<h5>Navigate to services:</h5>
										<ul>
											<li><a href="#iam">IAM</a></li>
											<li><a href="#vpc">VPC</a></li>
											<li><a href="#ec2">EC2</a></li>
											<li><a href="#rds">RDS</a></li>
											<li><a href="#s3">S3</a></li>
											<li><a href="#lambda">Lambda</a></li>
											<li><a href="#cloudfront">CloudFront</a></li>
											<li><a href="#storagegateway">Storage Gateway</a></li>
											<li><a href="#redshift">Redshift</a></li>
											<li><a href="#snowflake">Snowflake</a></li>
											<li><a href="#glue">Glue</a></li>
											<li><a href="#athena">Athena</a></li>
											<li><a href="#emr">Elastic MapReduce</a></li>
											<li><a href="#datalake">Data Lake</a></li>
											<li><a href="#cloudwatch">Cloud Watch</a></li>
											<li><a href="#route53">Route 53</a></li>
											<li><a href="#elasticache">ElastiCache</a></li>
										</ul>

										<!-- <h5>6. Continuous monitoring:</h5>
										<ul>
											<li>...</li>
											firewall configures some security rules. Accepting connection from trusted network and blocking connection from untrusted network.
										</ul> -->

									</header>
								</div>
							</section>

						<!-- Two -->
							<section id="two">
								<div class="container">
									<h3>Features</h3>
									<p>App includes following features:</p>
									<ul class="feature-icons">
										<li class="icon solid fa-check">AWS</li>
									</ul>
								</div>
							</section>

						<!-- Three -->
							<section id="three">
								<div class="container">
									<h3>Demo</h3>

									<h5 id="cloudcomputing">Cloud Computing:</h5>
									<ul>
										<li>Common opinion on cloud computing is that it concerns System Administrators or DevOps Engineers because of solutions it provides on servers provsion, networking etc.
											In fact, It also concerns Software Developers, Security Engineers, Program Managers and so on as there are many services related to computing, storage, database, analytics, encryption, deployment and more	.</li>
										<li>Cloud computing is basically a model in which computing resources are available as a service.</li>
										<li>Important characteristics of cloud computing:
											<br>
											- <u>On-demand and self-serviced</u>: we can launch it at any time with no manual intervention.
												What that means is we can provison resources whenever needed without requireing any human's action from provider's side.<br>
											- <u>Elasticity</u>: we can scale it up or down at any time due to our needs.
												This property is strictly related to scalability that can be horizontal (adding or removing servers in a cluster) or vertical (adding or removing resources in existing server).<br>
											- <u>Measured</u>: we pay as much as much resources we use.<br>
											- <u>High-Availability</u>: making backups and accommodating failure of a single component. I case of a server's failure there is another instance to back it up.
										</li>
										<li>There are 3 types of cloud computing:
											<br>
											- <b>Software as a Service (Saas)</b> - a ready-to-use application typically accessible via a browser. Provider takes responsibility for upgrading, security, error handling etc.<br>
											- <b>Platform as a Service (PaaS)</b> - deploying Python application on server along with all dependencies.<br>
											- <b>Infrastructure as a Service (Iaas)</b> - gives the highest level of flexibility and management control over entire IT infrastrstructure without having to physically maintain it.
												It provides access to networking features, computers (virtual or on dedicated hardware), and data storage space.<br>
											<br>
											<img src="images/service_models.png" width="600"><span style="font-size:12px">source: wp-includes.com</span>
										</li>
										<li>Cloud Architecture:
											<br>
											1. Clouds is actually a real Data Center with X servers already set up.<br>
											2. Data Center provides Virtualization layer (in case of AWS  it's mainly XEN).<br>
											3. On top of virtualization layer there are multiple Virtual servers.<br>
											<br>
											<img src="images/architecture.JPG" width="400">
										</li>
										<li>Data centers are organized into availability zones that are separated by geograpic region. They play a role of backups in case of one of the Data Centers failure.</li>
										<li>Each AWS region contains at least 2 availability zones.</li>
									</ul>

									<h5 id="serverconnection">Server Connection:</h5>
									<ul>
										<li>For server connection we need to get:
											<br>
											- <u>SSH Client</u> for Linux Server or<br>
											- <u>RDP Client</u> for Windows Server<br>
											which allows us to get connected with the server.
										</li>
										<li>To get SSH client for windows on local machine I use MobaXterm.</li>
										<li>MAC and Linux has its own terminal with already built-in SHH client.</li>
										<li>There is also a way of connecting to server from a browser using <b>Broweser Based SSH Connection</b>.
												Then you don'y have to have any SSG client. It can be done from AWS console directly.</li>
										<li>There is also a need for establishing the <b>Key Based Authentication</b>.
											<br>
											- it replaces the password based authentication as using password for authentication is less secure,<br>
											- in key based authentication there are two special keys: public key and private key,<br>
											- when public key is stored in a server then only corresponding private key can authenticate successfully,<br>
											- we can simply create a key pair for exampple in EC2 in the AWS console.
												We can choose its format as pem (when using OpenSSH) or ppk (when using PuTTY).
												Once created, key gets downloaded to local machine as the private key.<br>
										</li>
									</ul>

									<h5 id="iaac">Infrastructure as a Code (IaaC):</h5>
									<ul>
										<li>There are two ways of building the infrastructure: manually or with script's automation.</li>
										<li>Automation with Iaac helps to autmate infrastrucutre building on every stage of app's service:
											<br>
											1. As every time a new app's service comes up, its infrastructure needs to be first built up in the development environment.<br>
											2. Then app's service comes to the staging area for testing where the same infrastructure needs to be built once again.<br>
											3. Moving to the production environment where all the infrastructure has to be replicated.
										</li>
										<li>It's getting even more helpful when deploying multiple app's services.</li>
										<br>
										<img src="images/iaac.JPG" width="610"><span style="font-size:12px;">source: udemy</span>
										<li>One IaaC template can be reused for building infrastructure for different stages of different app's services during deployment.</li>
										<li>Tools for IaaC: Terraform or AWS CloudFormation.</li>
										<li>AWS CloudFormation:
											<br>
											- provides template where you describe your desired resources and their dependencies so that you can launch and configure them together as a stack.<br>
											- workflow:<br>
											<img src="images/cloud_formation.png" width="600"><span style="font-size:12px">source: AWS</span>
										</li>
									</ul>

									<h5 id="awsstorage">AWS Storage types:</h5>
									<table>
										<thead>
											<tr>
												<th>Block Storage</th>
												<th>Object Storage</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td>Data (files) is split into smaller chunks of a fixed size (blocks).</td>
												<td>Each store fiels is an object.</td>
											</tr>
											<tr>
												<td>Each block has its own address.</td>
												<td>Each object has unique identifier.</td>
											</tr>
											<tr>
												<td>No metadata about blocks.</td>
												<td>Metadata with contextual information about single object.</td>
											</tr>
											<tr>
												<td>Supports read/write operations.</td>
												<td>Data is mostly read (rather than written to).</td>
											</tr>
											<tr>
												<td>Easy data modification accessing specific block.</td>
												<td>Modifying a file means uploading a new revision.</td>
											</tr>
											<tr>
												<td>Accessing blocks on server with underlying file system protocol (NFS, CIFS, ext3/ext4).</td>
												<td>Accessing objects relies on HTTP protocol.</td>
											</tr>
										</tbody>
									</table>

									<h5 id="domainnamesystem">Domain Name System:</h5>
									<ul>
										<li>Translates Domain Name to corresponding server's IP address:
											<br>
											<code>www.example.com</code> -> <code>1.2.3.4</code>
										</li>
										<li>Workflow:
											<br>
											1. User enters <code>www.example.com</code> in a browser.<br>
											2. IPS DNS Resolver looks up corresponding IP address and returns <code>1.2.3.4</code>.<br>
											3. Browser takes server IP address and makes HTTP request to AWS server.<br>
											4. AWS EC2 server accepts request.
										</li>
										<br>
										<img src="images/dns.JPG" style="border: 1px solid #939393"><span style="font-size:12px;">source: AWS</span>
									</ul>

									<h5 id="serverlessservices">Serverless Services:</h5>
									<ul>
										<li>In fact, it doesn't mean that there is no server being present.
												There are server to host your application, however is completely managed by the provider.
												You only care about the app's code.</li>
										<li>Very popular way of having serverless service is a PaaS model where only need is to upload the the application.
												PaaS provider takes care of the rest which is setting up the capacity, launching servers in high-availability and auto-scaling mode, installing technology-specifc packages and dependencies, security, patching, monitoring.
										<li>There are AWS Services that can be used without any server instantiation.
												There is no need for any capacity planning or how much resources we might need.
												We are being charged only for the computing time we consume.
												Example serverless services:
											<br>
											- AWS Lmabda for computing,<br>
											- AWS S3 for storage,<br>
											- DynamoDB for Database, <br>
											- SQS, SNS for an app integration.
										</li>
									</ul>

									<h5 id="storageclasses">Storage classes:</h5>
									<ul>
										<li>Depending on Storage Class, the availablity, durability and perfomrance, thus <u>pricing</u> will differ:
											<br>
											<b>- Standard S3</b>: for general purpose, has higher availablility and pricing much higher than for infrequent access.<br>
											<b>- Standard S3 with Infrequent Access (Standard IA)</b>: when we don't care about high availablity then we can go with that opition with lower pricing.<br>
											<b>- Reduced Redundancy Storage (RRS)</b>: lower durability and lower availablity so we could keep only non-critical, reproducible data.<br>
											<b>- Glacier</b>: meant for archiving and storing long-term backups. It has a very high durability however low availability - it takes even a few hours to get data restored.<br>
											<b>- Glacier Deep Archive</b>: lowest-costs possible storage class that AWS offers. Supports long-term retention for data that may be accessed once or twiece a year.
												It has very lowe availablity - data can be restored within 12 hours.<br>
											<b>- Intelligent Tiering</b>: it detects seldom used data and moves it to most cost-effective tier like Standard IA.
												So we end up with frequent access tier and infrequent access tier that differ with pricing.
												This type is preferable when we store long-lived data where access patterns are unknown or unpredictable - we cannot assess which part of data will bea accessed frequently and which not.
												<br>
											<b>- One Zone-IA</b>: while Standard S3 or Standard IA sores data in min. 3 availability zones, S3 On Zone_IA stores data in single availablity zone which reduces overall costs.
												It's a good solution for a secondary backup copies of on-premises data or for the data that can be easily recreated. Only risk is the data will be lost in case of availablity zone destruction.
										</li>
										<li>We can choose storage class while uploading object to S3:
											<br><br>
											<img src="images/storage_classes.png" width="780">
										</li>
									</ul>

									<h5 id="iam">AWS Identity and Access Management (IAM)</h5>
									<ul>
										<li>Managing access and user roles to AWS services and resources.</li>
										<li>IAM as a feature is free of charge. You are only charged for use of AWS services by your users.</li>
										<li>Things we can do with IAM:
											<br>
											- creating users assigning them individual security credentials and providing access to AWS services and resources,<br>
											- managing user roles and permissions to control what operation can be perfomred by the individaul or what AWS resources the individual is allowed to access,<br>
										</li>
									</ul>

									<h5 id="vpc">Virtual Private Cloud (VPC):</h5>
									<ul>
										<li>It is a private sub-section of AWS which you are in control of in terms of who has access to what AWS resources.</li>
										<li>More technically, AWS lets us provision a logically isolated cloud's section in which we can define a custom virtual network configurations like IP addresses, subnets, route tables and network gateways.</li>
										<li>When creating AWS account, there is a default VPS being created for a user. So everybody has their own VPC.</li>
										<li>VPC architecture and components:
											<br>
											<img src="images/vpc.JPG" width="300"><span style="font-size:12px;">source: Linux Academy</span>
											<br>
											- Internet Gateway (IGW) allows communication between your VPC and the internet..<br>
											- A route table lists predefined routes to the default subnets.<br>
											- A Network Access control List has pre-defined rules for access.<br>
											- VPC is partitioned into subnets to provision AWS resources in (e.g. EC2 instances).
										</li>
										<li>We can set a specific VPC for an EC2 instance:
											<br><br>
											<img src="images/ec2_vpc.jpg" width="700">
										</li>
									</ul>

									<h5 id="ec2">EC2:</h5>
									<ul>
										<li>It stands for Elastic Computing Cloud.</li>
										<li>Actually, it's a name for a server that we can launch in AWS.</li>
										<li>Elastic means that we can resize the server's capacity at any time.</li>
										<li>AWS ensures high-availablity so when on EC2 server goes downm the hosted application can be still served on another EC2 server.</li>
										<li>Launching server is as easy as hitting a button and going through a configuration
											<br>
											<img src="images/ec2.JPG" width="780">
										</li>
										<li>When launching a new instance of EC2 on AWS we need to configure following things:
											<br>
											- region,<br>
											- server OS which is Amazon Machine Image (AMI),<br>
											- CPU and memory size of EC2 instance,<br>
											- num of instances,<br>
											- storage capacity,<br>
											- authentication key,<br>
											- security (firewall).
										</li>
										<li>Once instance created, we can connect server with SSH:
											<br>
											- connecting to EC2 server:<br>
											<code>ssh -i ec2-key.pem ec2-user@{Public IP}</code><br>
											- getting admin rights:<br>
											<code>sudo su -</code><br>
											- intalling some stuff:<br>
											<code>yum -y install nginx</code><br>
											<code>yum -y install mysql</code><br>
											- cd to location:<br>
											<code>cd /usr/share/nginx/html/</code><br>
											- modifying a file:<br>
											<code>echo "Hello World" > index.html</code><br>
											- running service:<br>
											<code>service nginx start</code>
										</li>
										<li>OS for EC2 instance is the <b>Amazon Machine Image (AMI)</b>. We can run multiple instances from a single AMI.</li>
										<li>There are also a persistent block storage volumes for AWS EC2 instances. We call it <b>Elastic Block Store (EBS)</b>:
											<br>
											- It's available under Root device property of the EC2 instance:<br>
											<br>
											<img src="images/ebs.jpg" width="700">
											<br>
											- Persistent means that the data will remain even when we stop the EC2 instance.<br>
											- The volumes are replicated, backed up and connected to EC2 instances with the network.<br>
											<br>
											<img src="images/datacenter.JPG" width="700">
											<br>
											- We can still ustilize the instance's store which gives fast performance, however the data will be lost if you EC2 instance stops or terminates or the underlying hosting disk fails.
												The recompensation might be the fact it's quite cost-effecitve, however, we need to make sure to back up the data in S3 for example.<br>
											- What kind of storage we want to use, we need to specify on AMI configuration step:<br>
												<br>
												<img src="images/instance_storage.JPG" width="700">
										</li>
										<li>EC2 is equipped with <b>Elastic Load Balancer (ELB)</b> so that traffic can be distributed across multiple EC2 instances.</li>
										<li>EC2 has <b>Auto-Scaling</b> built in which automatically adds or removes EC2 instances according to conditions we define e.g.:
											<br>
											1. Dynamic Scaling:<br>&nbsp&nbsp&nbsp
											- if average CPU utilization > 60 % then add two more instances,<br>&nbsp&nbsp&nbsp
											- if average CPU utilization < 30 % then remove two instances.<br>
											2. Scheduled Scaling:<br>&nbsp&nbsp
											- servers are scaled based on a specific schedule.<br>
											3. Predictive Scaling:<br>&nbsp&nbsp&nbsp
											- based on machine learning algorithms to automatically adjusting servers capacity.
										</li>
									</ul>

									<h5 id="rds">AWS Relational Database Service (RDS):</h5>
									<ul>
										<li>AWS RDS supports various database engines like MySQL, PostgreSQL, Microsoft SQL Server, Oracle that can be hosted on EC2.</li>
										<li>AWS offers also the noSQL databse in DynamoDB that stores key-value pairs.</li>
										<li>Like in other services, AWS provides:
											<br>
											- database provisioning via GUI,<br>
											- security,<br>
											- patching,<br>
											- backup,<br>
											- high-availablity.
										</li>
										<li>We are able to pick up the engine while creating database
											<br><br>
											<img src="images/rds.JPG" width="550">
										</li>
										<li>Amazon Aurora is a compromise between performance of traditional enterprise databases and simplicity and cost-effectiveness of open-source databases.
												Once creating we don't have to specify the storage as it grows along with the size of it.
										</li>
									</ul>

									<h5 id="s3">Amazon Simple Storage Service (Amazon S3):</h5>
									<ul>
										<li>S3 is the durable storage system and is based on object storage.</li>
										<li>In S3 we have buckets that are like folders where we can store multiple objects (files).
												Bucket names are unique across enitre AWS namespace.
												Buckets can have subfolders.
										</li>
										<li>Can be used for storing simple websites at the lower costs. With that solution there is no need for instantiating EC2 server.</li>
										<li>When you upload some files to the cloud storage it backs file up automatically.</li>
									</ul>

									<h5 id="lambda">AWS Lambda:</h5>
									<ul>
										<li>It's a fully managed compute service that runs our code when event appears (for instance uploading objects to S3 can trigger Lambda function)or on the time base.</li>
										<li>AWS Lambda provides:
											<br>
											- servers,<br>
											- capacity,<br>
											- deployment,<br>
											- scaling,<br>
											- high-availability,<br>
											- os updates,<br>
											- security.
										</li>
										<li>What we provide:
											<br>
											- code,<br>
											- money - as much as we use it.
										</li>
										<li>There are many AWS-related events that can trigger a lambda function:
											<br><br>
											<img src="images/lambda_triggers.png" width="550">
										</li>
										<li>We can specify Runtime while configuration:
											<br><br>
											<img src="images/runtime.JPG" width="550">
										</li>
									</ul>

									<h5 id="cloudfront">AWS CloudFront:</h5>
									<ul>
										<li>Service for Content Delivery Network (CDN) which acts like a proxy that receives requests and forward those requests to the backend systems.</li>
										<li>CDN caches website or application files, HTML, CSS, JS, images or videos at data centers around the world.
												Even when backend server goes down, CDN is able to serve the content of a static web-site back to the end user..</li>
										<li>When setting up the CloudFront service we define amount of data centers - <b>the edge locations</b>.
												The more edge locations, the higher perfomrance and the lower latency of getting server's content as reposnse on user's request.</li>
										<li>The edge locations allow users to download the app content much faster from the nearest edge location
												rather than when request would need to go all the way to the origin server.
										</li>
										<li>User's request may need to go to the origin server in case when the content at the closest edge location is not present at the moment.</li>
										<li>Content is being cached at the edge location for a specific period of time - <b>Time To Live (TTL)</b>.</li>
									</ul>

									<h5 id="storagegateway">AWS Storage Gateway:</h5>
									<ul>
										<li>A service that lets the on-premise application to access and use the cloud storage.</li>
										<br>
										<img src="images/storage_gateway.JPG" width="600"><span style="font-size:12px;">source: AWS</span>
										<br>
										<li>In <b>Gateway Stored Volumes</b> configuration, there is on-premise storage for an application server.
												Whenever a file added to that special local storage, it's being uploaded asynchronously to the AWS S3 or AWS EBS in a compressed manner.
										</li>
										<li>In <b>Gateway Cached Volumes</b> configuration, there is no on-premises storage.
												Data is stored primarly on AWS S3, what we have locally on-premise server is a cache of recently read or writthen data.
										</li>
									</ul>

									<h5 id="redshift">Data Warehouse:</h5>
									<ul>
										<li>Using in Business Intelligence in which we transofrm raw data into useful business insights. It goes with following steps:
											<br>
											1. Gathering data from different sources: ERP, CRM, OS, flat files.<br>
											2. Extracting data from all sources, transforming and data cleaning and loading it into a data warehouse.<br>
											3. Using data in data warehouse for the business analysis.
										</li>
										<br>
										<img src="images/data_warehouse.JPG" width="600"><span style="font-size:12px;">source:radikal-labs.com</span>
										<li>Data ralational database vs data warehouse:
											<br><br>
											<table>
												<thead>
													<tr>
														<th>Relational Database</th>
														<th>Data Warehouse</th>
													</tr>
												</thead>
												<tbody>
													<tr>
														<td>Contains the up-to-date data.</td>
														<td>Contains the historical data.</td>
													</tr>
													<tr>
														<td>Useful in running the business.</td>
														<td>Useful in analyzing the business.</td>
													</tr>
													<tr>
														<td>Read and write operations.</td>
														<td>Mostly read opeartions.</td>
													</tr>
													<tr>
														<td>Accessing limited number of records.</td>
														<td>Accessing even milion of rows if needed.</td>
													</tr>
													<tr>
														<td>Usually one source that serves an application.</td>
														<td>Typically a collection of many data sources.</td>
													</tr>
												</tbody>
											</table>
										</li>
										<li>AWS offer data warehouse service with <b>Amazon Redshift</b> whose architecture looks like below:
											<br><br>
											<img src="images/redshift.JPG" width="600"><span style="font-size:12px;">source: simplilearn</span>
											<br>
											- The main element of Amazon Redshift is a cluster of nodes - datawarehouse cluster.<br>
											- There are <b>compute nodes</b> that process data and the <b>leader node</b> that gives instructions.<br>
											- Leader node also manages <b>client applications</b>, like BI tools, that require data from the Redshift.<br>
											- Leader node uses <b>JDBS (Java Database Connectivity)</b> to monitor all of the connections to client appications.<br>
											- Client applications uses <b>ODBC (Open Database Connectivity)</b> to interact with live data of the datawarehouse cluster sending SQL queries.<br>
											- Compute nodes are divided into slice with dedicated memory space. They run in parallel to process the data in a fast manner.<br>
											- Workflow:
											<br>
											1. Client app sends query to the leader node.<br>
											2. Leader node receives the query and develop a suitable execution plan.<br>
											3. Once the plan is set up, compute nodes and compute slices start working on this plan.<br>
											4. Compute nodes work in parallel and transfer data among themeselves in order to solve the query.<br>
											5. Once excution is done. the leader node aggregates the results and sends it backt to clien app.
									 </li>
									</ul>

									<h5 id="snowflake">Snowflake:</h5>
									<ul>
										<li>Snowflake is the platform delivered as a service which means we don't have to worry about patches, upgrades or maintenance.</li>
										<li>We can utilize AWS S3 as the data storage for OLAP in Snowflake.</li>
										<li>I like the way they describe it: <i>combines the power of data warehousing, the flexibility of big data platforms, and the elasticity of the cloud</i>.</li>
										<li>In Snowflake's architecture, the key is a separation of storage, compute and services:
											<br><br>
											<img src="images/snowflake_architecture.png" width="640"><span style="font-size:12px">soruce: snowflake</span>
											<br>
											- S3 as a storage layer (Google Cloud Storage or Azure Blob Storage),<br>
											- EC2 Virtual Warehouse as a compute layer .<br>
											- metadata, security, transactions and concurrency management as service layer.
										</li>
										<li>Building pipelines with Snowflake:
											<br><br>
											<img src="images/snowflake_pipeline.JPG">
											<span style="font-size:12px">soruce: snowflake</span>
										</li>
										<li>Snowflake for OLAP:
											<br>
											- snowflake supports <b>Online Analytical Processing (OLAP)</b> for scanning all of the data in columns in order to get aggregated values as opposed to <b>Online Transactional Processing (OLTP)</b> where we want to retrieve one or many rows from database.
												The main purpose of OLAP is data analysis and when it comes to OLTP then main purpose is data processing (inserting, updating).
											<br><br>
											<img src="images/olap.JPG" style="border: 1px solid gray;" width="420"><span style="font-size:12px">soruce: Data Council</span>
											<br>
											- while a classic databases like PostgreSQL or MySQL are type of <b>raw store</b> (OLTP) that keeps data linearly row by row, from left to right, from top to bottom, Snowflake supports <b>columnar store</b> (OLAP) which allows to traverse entire columns one by one.
											<br><br>
											<img src="images/columnstore.JPG" style="border: 1px solid gray;" width="640"><span style="font-size:12px">soruce: Data Council</span>
											<br>
											- using columnar storage makes sense when we want to analyze one or two specific columns and skip reading other ones.
										</li>
										<li>Snowflake workflow:
											<br>
											- Dividig big tabular data into x-consecutive rows segments called micropartitions.<br>
											- Reorganizing the data of each micropartition by flipping row store into columnar - column values of each row get stored together.<br>
											- Adding a header to each micropartition. Header contains metadata of given micropartition and columns it contains.<br>
											- Micropartitions are being stored in S3 as the immutable files.<br>
											- All the information about micropartitions are being stored in Sknowflake's metadata layer.<br>
											- When query comes in, metadata layer knows which micropartition in S3 belongs to the queried table.<br>
											- Reading a S3 file, it searches for a specific header first and then reads the only column we specified in the query.<br>
											- It reads the only queried data ingnoring the rest of S3 files.
										</li>
										<li>Snowflake architecture:
											<br>
											- Database - containing schemas, tables, views, stages, file formats, sequences. All the tables are created in PUBLIC schema by default.<br>
											- Stage - a data lake like place from which data files are being copied into the tables.<br>
											- File format - we need to define fomrat of the files bieng stored at stage.<br>
											- Warehouse - computing enginge.<br>
											- Worksheet - a place where we can proceed with writing or loading ready SQL scripts.<br>
										</li>
										<li>Every action on db can be performed with sql script along with setting up stages, file formats,
												configuring warehouse, copying data to a table from a stage.
												<br><br>
												<img src="images/sql_script.JPG">
												<br>
												<img src="images/sql_script2.JPG">
										</li>
										<li>Data Loading consists of two parts:
											<br>
											1. Staging the files into the place where Snowflake can retrieve data files from. We cam have both internal and exteranl (S3, GSP) storages.<br>
											2. Loding the data into a specific table.
										</li>
										<li>SnowSQL:
											<br>
											- the command line interface client for connecting to Snowflake,<br>
											- allows to execute SQL queries to perform DDL and DML operations,<br>
											- we can install it from Snowflake UI and there is info on how to connect at the end:
											<br><br>
											<img src="images/snowsql_installation.JPG">
											<br>
											- example:
											<br>
											<code>USE DATABASE my_db;</code><br>
											<code>CREATE OR REPLACE TABLE my_tbl {</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>...</code><br>
											<code>};</code><br>
											<code>CREATE STAGE csvfiles;</code><br>
											<code>PUT file:///tmp/load/abc*.csv @csvfiles;</code><br>
											<code>LIST @csvfiles;</code><br>
											<code>CREATE WAREHOUSE IF NOT EXISTS "dataload" WAREHOUSE.SIZE = "Large"</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>AUTOSUSPEND = 300 AUTO_RESUME = TRUE;</code><br>
											<code>USE WAREHUSE dataload;</code><br>
											<code>COPY INTO my_tbl</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>FROM @csvfiles</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>PATTERN = '.*file0[1-4].csv.gz'</code><br>&nbsp&nbsp&nbsp&nbsp
												<code>ON_ERROR = 'skip_file';</code><br>
											<code>SELECT * FROM my_tbl LIMIT 10;</code>
										</li>
										<li>We can learn a lot from the documentation:
												<u><a href="https://docs.snowflake.com/en/">Snowflake documentation</a></u>
										</li>
									</ul>

									<h5 id="glue">AWS Glue:</h5>
									<ul>
										<li>ETL service to categorize, clean, enrich, and reliably move data between various data stores.</li>
										<li>It collects different data carriers, then identifies the data types, suggests some transformations and generates editable code (ETL script) to execute the overall transformation and datawarehouse loading process.</li>
										<li>AWS Glue has 3 main components:
											<br>
											- <b>Data Catalog</b> - central metadata repository that alwyas stays in sync with the underlying data thanks to the so-called <b>crawlers</b>.<br>
											- <b>Job Authoring</b> - ETL engine that automatically generates Python or Scala code.<br>
											- <b>Job Execution</b> - flexible scheduler that handles dependency resolution, job monitoring, potential retries and alerting.
										</li>
										<li>ETL scripts uses the <b>dynamic frame</b> - similar to the Apache Spark dataframe.
												Dynamic frame is a data abstraction to organize data into rows and columns where each record is self-describing so no schema is required initially.
												We can freely be converting dynamic frames into Spark dataframes.
										</li>
										<li>Here are some applicatons:
												<br>
												- AWS Glue can catalog S3 data lake making it available for quering with Amazon Athena and Amazon Redshift.<br>
												- Making event-driven ETL pipelines: running ETL jobs as soon as new data comes in Amazon S3 by invoking your AWS Glue ETL jobs from an AWS Lambda function.<br>
												- Cataloging data for quick search of datasets and maintaining relevant metadata in once central repository.
										</li>
									</ul>

									<h5 id="athena">AWS Athena:</h5>
									<ul>
										<li>Query service for managing Amazon S3 data with standard SQL.</li>
										<li>There is no need for any underlying compute infrastructure, no need for loading data into Amazon Athena or transforming it for the analysis.</li>
										<li>We can access Athena through either the <b>AWS Management Console</b>, an application programming interface (API) or a Java Database Connectivity driver, then we we define schema and here we go - we can execute SQL queires.</li>
										<li>You only pay for queries you run and that's all.</li>
									</ul>

									<h5 id="emr">AWS Elastic MapReduce (EMR):</h5>
									<ul>
										<li>Platform for computational processing of vast amounts of data with the help of the MapReduce framework. It simplifies the enitre setup and management of the cluster and Hadoop components.</li>
										<li>Hence, It uses Hadoop to distribute the data and process it across an auto-scaling cluster of computing nodes (EC2 instances).</li>
										<li>EMR continously monitors nodes in the cluster. It retrieves failed tasks and replaces poorly performing instances.</li>
										<li>We can choose computation engine while cluster establishing:
											<br><br>
											<img src="images/emr.JPG" width="700">
										</li>
									</ul>

									<h5 id="datalake">Data Lake:</h5>
									<ul>
										<li>Data Lake vs Data Warehouse
											<br><br>
											<table>
												<thead>
													<tr>
														<th>Data Lake</th>
														<th>Data Warehouse</th>
													</tr>
												</thead>
												<tbody>
													<tr>
														<td>Stores raw, unstructured and unprocessed data.</td>
														<td>Stores refined, structured and processed data.</td>
													</tr>
													<tr>
														<td>Stores data that may never be used hence larger storage capacity required.</td>
														<td>Saves storage space by not maintaining data that may never be used.</td>
													</tr>
													<tr>
														<td>Poor data quality.</td>
														<td>Data quality ensured.</td>
													</tr>
													<tr>
														<td>Purpose of data gathering not determined.</td>
														<td>Data being gathered for a specific business purpose.</td>
													</tr>
													<tr>
														<td>Easy accessible and quick to update because of the lack of structures	.</td>
														<td>More complicated and costly to make changes.</td>
													</tr>
													<tr>
														<td>Used by Data Scientists.</td>
														<td>Used by Business Analysts.</td>
													</tr>
													<tr>
														<td>Requires specialized tools like Machine Learning to understand and translate data into usage.</td>
														<td>Can be used with regular Business Intelligence tools to visualize data with charts, tables.</td>
													</tr>
												</tbody>
											</table>
										</li>
										<li>Data Warehouse based on AWS services:
											<br><br>
											<img src="images/data_warehouse_aws_services.png" width="600"><span style="font-size:12px">source: AWS</span>
											<br>
											- data ingestion - Amazon Kinesis Data Firehose.<br>
										  - data storage - Amazon S3.<br>
											- data processing - AWS Lambda and AWS Glue.<br>
											- data migration - AWS Data Migration Service (AWS DMS) and AWS Glue.<br>
											- orchestration and metadata management - AWS Glue for.<br>
											- querying and data visualization Amazon Athena and Amazon QuickSight.
										</li>
										<li>Data Lakes in AWS:
											<br>
											- Offers more agility and flexibility than traditional data management systems.<br>
											- Allows companies to store all of their data from various sources, regardless if they are structured and unstructured, in a centralized repository.<br>
											- Configures the core AWS services to easily tag, search, share, and govern subsets of data across a company.<br>
											- Stores and registers datasets of any size in the secure, durable, scalable AWS S3.<br>
											- Allows users to upload and catalog new datasets with searchable metadata and integrate it with AWS Glue and Amazon Athena to transform and analyze.<br>
											- Crawls the data sources, idetifies data formats, and then suggests scheams and transofmrations with no hand-coding data flows.<br>
											- Adds user-defined tags into AWS DynamoDB to add business-relevant context to each dataset.<br>
											- Allows users to browse available datasets or search on dataset attributes and tags to quickly find and access data relevant to their business needs.
											</li>
										<li>Data Lake based on AWS services:
											<br><br>
											<img src="images/datalakes.png" width="600"><span style="font-size:12px;">source: AWS</span>
											<br>
											- The data lake infrastructure can be provisioned by AWSC CloudFormation.<br>
											- Data lake API leverages Amazon API Gateway to provide access to data lake microservices through AWS Lambda functions.<br>
											- These microservices interact with Amazon S3, AWS Glue, AWS Athena, AWS DynamoDB, AWS ES, and AWS CloudWatch Logs to provide data storage, management, and audit functions.<br>
										</li>
									</ul>

									<h5 id="cloudwatch">AWS CloudWatch:</h5>
									<ul>
										<li>Monitoring service fro AWS servers and applications.</li>
										<li>It collects and monitors log files, sets alarms, reacts to changes in the AWS resources automatically, for example:
												when CPU utilization in an EC2 instance is grater than 70 % then you get the email alarming notification.<br>
										</li>
										<li>AWS CloudWatch Logs:
												<br>
												- Server can keep a lot of log files both sytem and application logs.<br>
												- It is iportant to have a log files during application's debugging. If there is something that doesn't work as expected then we would need to check the errors in the specific log file.<br>
												- In traditional way, when debugging we need to grant access to the server for an individual who wants to check a log file.
													On the other hand, what poses the risk is that when server termiantes, the logs are lost.<br>
													- The better way is to create a central logs server in which we want to push the log files from individual systems. Then we can go with the central log monitoring.<br>
													- AWS CloudWatch Logs is the <b>centralized logs management</b> to monitor, store, and access log files form Amazon EC2 instances, Route 53 and other sourcers.<br>
											</li>
									</ul>

									<h5 id="route53">AWS Route 53:</h5>
									<ul>
										<li>Domain Name System (DNS) web service used to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. </li>
										<li>Amazon Route 53 connects user requests to infrastructure running in AWS (EC2, S3) or routes users to infrastructure outside of AWS.</li>
										<li>With AWS Route 53 I can route traffic to differnet app's endpoints idependently and monitor their health.</li>
										<li>It is not limited to a specific region. We can launch it globally.</li>
									</ul>

									<h5 id="elasticache">AWS ElastiCache:</h5>
									<ul>
										<li>In-memory cache in the cloud. It caches the response associated with frequent queries.</li>
										<li>This allows better response time and decreases the load on the database server.</li>
										<li>When user sents the same query once again, the response comes from cache engine instead of from database server.</li>
									</ul>

									<!-- <h5>Message Broker:</h5>
									<ul>
										<li></li>
									</ul>-->

								</div>
							</section>

						<!-- Four -->
							<section id="four">
								<div class="container">
									<h3>Setup</h3>

									<h5>Following installation required:</h5>
									<ul>
										<li>MobaXterm from https://mobaxterm.mobatek.net/download-home-edition.html</li>
									</ul>

								</div>
							</section>

							<section id="five">
								<div class="container">
									<h3>Source Code</h3>
									<p>You can view the source code: <a href="#">HERE</a></p>
									<p>&nbsp</p>
								</div>
							</section>

					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<a href="https://arturskrzeta.github.io/" style="padding-bottom:10px;">Back to Portfolio</a>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
